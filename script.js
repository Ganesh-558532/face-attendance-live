// =========================================================================
// 1. HTML Elements and Variable Declarations
// =========================================================================

const startCameraBtn = document.getElementById('startCameraBtn');
const takePhotoBtn = document.getElementById('takePhotoBtn');
const videoFeed = document.getElementById('videoFeed');
const photoCanvas = document.getElementById('photoCanvas');
const studentNameInput = document.getElementById('studentName');
const studentList = document.getElementById('studentList');

// NEW: File Upload Reference
const imageUpload = document.getElementById('imageUpload');

// Attendance button and result display elements are created dynamically
const markAttendanceBtn = document.createElement('button');
markAttendanceBtn.textContent = 'Mark Attendance (from Camera/File)'; // Translated
markAttendanceBtn.id = 'markAttendanceBtn';
markAttendanceBtn.style.display = 'none'; 
const controlsDiv = document.querySelector('.controls');
if (controlsDiv) {
Â  Â  controlsDiv.appendChild(markAttendanceBtn);
}

const attendanceResult = document.createElement('p');
attendanceResult.id = 'attendanceResult';
const containerDiv = document.querySelector('.container');
if (containerDiv) {
Â  Â  containerDiv.appendChild(attendanceResult);
}

let stream = null; 
let labeledFaceDescriptors = []; 
const DISTANCE_THRESHOLD = 0.7; 


// =========================================================================
// 2. Models Load And Setup
// =========================================================================

async function loadModels() {
Â  Â  if (attendanceResult) {
Â  Â  Â  Â  attendanceResult.textContent = "Models loading... please wait."; // Translated
Â  Â  }
Â  Â  
Â  Â  try {
Â  Â  Â  Â  await faceapi.nets.ssdMobilenetv1.loadFromUri('./models'); 
Â  Â  Â  Â  await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
Â  Â  Â  Â  await faceapi.nets.faceRecognitionNet.loadFromUri('./models');
Â  Â  Â  Â  
Â  Â  Â  Â  if (attendanceResult) {
Â  Â  Â  Â  Â  Â  attendanceResult.textContent = "Models loaded successfully! âœ…"; // Translated
Â  Â  Â  Â  }
Â  Â  Â  Â  if (markAttendanceBtn) {
Â  Â  Â  Â  Â  Â  markAttendanceBtn.style.display = 'inline-block'; 
Â  Â  Â  Â  }
Â  Â  } catch (error) {
Â  Â  Â  Â  console.error("Error loading face-api models. Check ./models folder and file integrity:", error);
Â  Â  Â  Â  if (attendanceResult) {
Â  Â  Â  Â  Â  Â  attendanceResult.textContent = "ERROR: Models failed to load. Check console and files. (Live Server required)"; // Translated
Â  Â  Â  Â  }
Â  Â  }
}


// =========================================================================
// 3. Functions (unchanged logic)
// =========================================================================

function displayStudent(name, photo) {
    // ... (unchanged display logic)
Â  Â  if (!studentList) return; 
Â  Â  const card = document.createElement('div');
Â  Â  card.classList.add('student-card');
Â  Â  const img = document.createElement('img');
Â  Â  img.src = photo;
Â  Â  img.alt = name;
Â  Â  const nameText = document.createElement('span');
Â  Â  nameText.classList.add('name');
Â  Â  nameText.textContent = name;
Â  Â  card.appendChild(img);
Â  Â  card.appendChild(nameText);
Â  Â  studentList.appendChild(card);
}

async function loadStudents() {
    // ... (unchanged loadStudents logic)
Â  Â  labeledFaceDescriptors = []; 
Â  Â  let students = JSON.parse(localStorage.getItem('students')) || [];
Â  Â  
Â  Â  if (studentList) { 
Â  Â  Â  Â  studentList.innerHTML = '<h2>Captured Students</h2>'; 
Â  Â  }

Â  Â  students.forEach(student => {
Â  Â  Â  Â  displayStudent(student.name, student.photo);
Â  Â  Â  Â  
Â  Â  Â  Â  if (student.descriptor) {
Â  Â  Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  Â  Â  const descriptor = new Float32Array(student.descriptor);
Â  Â  Â  Â  Â  Â  Â  Â  const labeledDescriptor = new faceapi.LabeledFaceDescriptors(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  student.name, 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  [descriptor]
Â  Â  Â  Â  Â  Â  Â  Â  );
Â  Â  Â  Â  Â  Â  Â  Â  labeledFaceDescriptors.push(labeledDescriptor);
Â  Â  Â  Â  Â  Â  } catch (e) {
Â  Â  Â  Â  Â  Â  Â  Â  console.error("Error converting descriptor for student:", student.name, e);
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
Â  Â  });
}


function saveStudent(name, photo, descriptor) {
    // ... (unchanged saveStudent logic)
Â  Â  let students = JSON.parse(localStorage.getItem('students')) || [];
Â  Â  const descriptorArray = Array.from(descriptor); 
Â  Â  
Â  Â  students.push({ name: name, photo: photo, descriptor: descriptorArray });
Â  Â  localStorage.setItem('students', JSON.stringify(students));
Â  Â  displayStudent(name, photo); 

Â  Â  loadStudents(); 
}

// =========================================================================
// 4. Event Listeners (Modified for File Upload)
// =========================================================================

document.addEventListener('DOMContentLoaded', () => {
Â  Â  loadModels();
Â  Â  loadStudents();
});

// Camera on event (same logic)
startCameraBtn.addEventListener('click', () => {
Â  Â  navigator.mediaDevices.getUserMedia({ video: true })
Â  Â  Â  Â  .then(videoStream => {
Â  Â  Â  Â  Â  Â  stream = videoStream;
Â  Â  Â  Â  Â  Â  videoFeed.srcObject = stream;
Â  Â  Â  Â  Â  Â  videoFeed.play();
Â  Â  Â  Â  Â  Â  takePhotoBtn.style.display = 'inline-block'; 
Â  Â  Â  Â  Â  Â  takePhotoBtn.textContent = 'Capture from Camera'; // Re-confirm button text
Â  Â  Â  Â  })
Â  Â  Â  Â  .catch(err => {
Â  Â  Â  Â  Â  Â  console.error("Camera access error: ", err); // Translated
Â  Â  Â  Â  Â  Â  alert("Camera access denied. Please check permissions."); // Translated
Â  Â  Â  Â  });
});


// ENROLLMENT LOGIC (Now handles both Camera and File Upload)
takePhotoBtn.addEventListener('click', async () => {
Â  Â  const studentName = studentNameInput.value.trim();
Â  Â  if (!studentName) {
Â  Â  Â  Â  alert("Please enter the student's name first!"); // Translated
Â  Â  Â  Â  return;
Â  Â  }

    // --- LOGIC FOR CAMERA CAPTURE ---
Â  Â  const context = photoCanvas.getContext('2d');
Â  Â  photoCanvas.width = videoFeed.videoWidth;
Â  Â  photoCanvas.height = videoFeed.videoHeight;
Â  Â  context.drawImage(videoFeed, 0, 0, photoCanvas.width, photoCanvas.height);

    const capturedImage = photoCanvas; 
    await processAndSaveFace(capturedImage, studentName);
});


// NEW: FILE UPLOAD Enrollment/Attendance Trigger (on change event)
imageUpload.addEventListener('change', async (event) => {
    const files = event.target.files;

    if (!files.length) {
        return;
    }
    
    // Check if the studentName is entered for enrollment (using the first file)
    const studentName = studentNameInput.value.trim();
    const isEnrollment = studentName && files.length === 1;

    if (isEnrollment) {
        // --- LOGIC FOR FILE ENROLLMENT ---
        const file = files[0];
        const img = await faceapi.bufferToImage(file); // Load the file into an image element
        
        await processAndSaveFace(img, studentName);
        
        // Clear inputs after successful enrollment
        studentNameInput.value = '';
        imageUpload.value = null; 

    } else if (files.length > 0) {
        // Attendance mode: Files are selected, now prompt user to click the attendance button
        alert(`Selected ${files.length} file(s). Now click 'Mark Attendance' button.`);
    }
});


// Reusable function to process image/canvas and save face
async function processAndSaveFace(mediaElement, studentName) {
    let img;
    // faceapi.nets.loadFromUri accepts HTML elements directly, but for canvas/image element, 
    // we should create a tensor first for consistent descriptor extraction
    
    // We create a temporary canvas to draw the media element for consistent tensor conversion
    const tempCanvas = document.createElement('canvas');
    const tempCtx = tempCanvas.getContext('2d');
    tempCanvas.width = mediaElement.naturalWidth || mediaElement.videoWidth || mediaElement.width;
    tempCanvas.height = mediaElement.naturalHeight || mediaElement.videoHeight || mediaElement.height;
    tempCtx.drawImage(mediaElement, 0, 0, tempCanvas.width, tempCanvas.height);

    img = faceapi.tf.browser.fromPixels(tempCanvas);
    
    const faceDetection = await faceapi.detectSingleFace(img)
        .withFaceLandmarks()
        .withFaceDescriptor();

    if (!faceDetection) {
        alert("No face detected in the image/camera feed. Please try again."); // Translated
        img.dispose();
        return;
    }

    const imageDataUrl = tempCanvas.toDataURL('image/png'); 
    saveStudent(studentName, imageDataUrl, faceDetection.descriptor);
    img.dispose();
}


// ATTENDANCE LOGIC (Now handles files if selected)
if (markAttendanceBtn) {
Â  Â  markAttendanceBtn.addEventListener('click', async () => {
Â  Â  Â  Â  if (labeledFaceDescriptors.length === 0) {
Â  Â  Â  Â  Â  Â  alert("Please capture student photos and descriptors first!"); // Translated
Â  Â  Â  Â  Â  Â  return;
Â  Â  Â  Â  }

        let attendanceSource = videoFeed; // Default to video feed

        const files = imageUpload.files;
        if (files.length > 0) {
            // --- LOGIC FOR FILE ATTENDANCE ---
            const file = files[0];
            attendanceSource = await faceapi.bufferToImage(file); // Load file into Image element
        }

Â  Â  Â  Â  const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, DISTANCE_THRESHOLD);

        // Detect faces from the loaded image or videoFeed
Â  Â  Â  Â  const results = await faceapi.detectAllFaces(attendanceSource)
Â  Â  Â  Â  Â  Â  .withFaceLandmarks()
Â  Â  Â  Â  Â  Â  .withFaceDescriptors();

        // If attendanceSource was a file image, we dispose of it's memory after detection
        if (files.length > 0) {
             // In browser environments, dispose is typically only needed for Tensors, 
             // but to be safe, we release resources if possible.
             // We'll rely on face-api's internal memory management for the image tensor.
        }

Â  Â  Â  Â  let presentStudents = new Set();
Â  Â  Â  Â  
Â  Â  Â  Â  results.forEach(detection => {
Â  Â  Â  Â  Â  Â  const bestMatch = faceMatcher.findBestMatch(detection.descriptor);
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  if (bestMatch.label !== 'unknown') {
Â  Â  Â  Â  Â  Â  Â  Â  presentStudents.add(bestMatch.label);
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  });

Â  Â  Â  Â  if (attendanceResult) {
Â  Â  Â  Â  Â  Â  if (presentStudents.size > 0) {
Â  Â  Â  Â  Â  Â  Â  Â  const names = Array.from(presentStudents).join(', ');
Â  Â  Â  Â  Â  Â  Â  Â  attendanceResult.innerHTML = `**Present Students (${presentStudents.size}):** ${names} âœ…`; // Translated
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  attendanceResult.innerHTML = "No recognized face found. ðŸ˜ž"; // Translated
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
        
        // Clear files selection after attendance
        imageUpload.value = null;
Â  Â  });
}

// ... (loadStudents and saveStudent remain the same)